{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Social_Distancing_Detection.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOGCP35Qz/cvZvAVvFA28Dj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Rt1rQsXNPGWt"},"source":["%%capture\n","# After this cell executes runtime will restart to finish the install, ignore and close the crash message, continue running cells starting with the one below\n","!pip install numpy==1.17.5;\n","!pip install tensorflow-gpu==1.15.0;\n","import os\n","os.kill(os.getpid(), 9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJDdOPwOPULp","executionInfo":{"status":"ok","timestamp":1622047203322,"user_tz":-480,"elapsed":3287,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}},"outputId":"fb463072-9228-44ac-c5d1-8b3ff97849e1"},"source":["%tensorflow_version 1.x\n","!pip install tf_slim"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Collecting tf_slim\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n","\u001b[K     |████████████████████████████████| 358kB 7.3MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (0.12.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n","Installing collected packages: tf-slim\n","Successfully installed tf-slim-1.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2gIT5YlQGh3i","executionInfo":{"status":"ok","timestamp":1622047216391,"user_tz":-480,"elapsed":6726,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}},"outputId":"05640fe1-857e-4d21-e544-c48317c02be6"},"source":["# Check if notebook is using GPU, and list the devices used by the notebook\n","from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[name: \"/device:CPU:0\"\n"," device_type: \"CPU\"\n"," memory_limit: 268435456\n"," locality {\n"," }\n"," incarnation: 9603202912170430017, name: \"/device:XLA_CPU:0\"\n"," device_type: \"XLA_CPU\"\n"," memory_limit: 17179869184\n"," locality {\n"," }\n"," incarnation: 3066533790470136477\n"," physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n"," device_type: \"XLA_GPU\"\n"," memory_limit: 17179869184\n"," locality {\n"," }\n"," incarnation: 2334926474006652357\n"," physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n"," device_type: \"GPU\"\n"," memory_limit: 14949928141\n"," locality {\n","   bus_id: 1\n","   links {\n","   }\n"," }\n"," incarnation: 8958768007063774491\n"," physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"AmOSJLlgPUag","executionInfo":{"status":"ok","timestamp":1622047218809,"user_tz":-480,"elapsed":359,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}}},"source":["# Images should contain the objects of interest at various scales, angles, lighting conditions, locations\n","# This configuration had results of 0.7 for mAP@0.5 for batch size 8 and number of steps 20k which is quite decent\n","num_steps = 20000  # A step means using a single batch of data. larger batch, less steps required\n","# Number of evaluation steps.\n","num_eval_steps = 50\n","# Batch size 24 generally works well. can be changed higher or lower \n","# Tested with 8, 16, 24, 32, and 64 for the batch size with 8 being the best\n","MODELS_CONFIG = {\n","        'ssd_mobilenet_v2': {\n","        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n","        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n","        'batch_size': 8\n","    }\n","}\n","selected_model = 'ssd_mobilenet_v2'\n","\n","# Name of the object detection model to use.\n","MODEL = MODELS_CONFIG[selected_model]['model_name']\n","\n","# Name of the pipline file in tensorflow object detection API.\n","pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n","\n","# Training batch size fits in Colab's GPU memory for selected model.\n","batch_size = MODELS_CONFIG[selected_model]['batch_size']"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bAIUKWJmPVIz","executionInfo":{"status":"ok","timestamp":1622047243754,"user_tz":-480,"elapsed":12020,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}},"outputId":"6c089780-9329-42bc-aded-18939473394b"},"source":["# Cloning the object detection demo flow repository\n","repo_url = 'https://github.com/GotG/object_detection_demo_flow'\n","import os\n","%cd /content\n","repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n","!git clone {repo_url}\n","%cd {repo_dir_path}\n","!git pull"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content\n","Cloning into 'object_detection_demo_flow'...\n","remote: Enumerating objects: 3035, done.\u001b[K\n","remote: Total 3035 (delta 0), reused 0 (delta 0), pack-reused 3035\u001b[K\n","Receiving objects: 100% (3035/3035), 229.08 MiB | 33.13 MiB/s, done.\n","Resolving deltas: 100% (1375/1375), done.\n","Checking out files: 100% (2796/2796), done.\n","/content/object_detection_demo_flow\n","Already up to date.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KqAwlC2FPVTH","executionInfo":{"status":"ok","timestamp":1622047251514,"user_tz":-480,"elapsed":3605,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}},"outputId":"6da384cf-d214-49b1-a4a2-d03d73f494dd"},"source":["%cd /content\n","!git clone https://github.com/AVCTY/inference_saved_model.git\n","!git pull"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content\n","Cloning into 'inference_saved_model'...\n","remote: Enumerating objects: 259, done.\u001b[K\n","remote: Counting objects: 100% (259/259), done.\u001b[K\n","remote: Compressing objects: 100% (252/252), done.\u001b[K\n","remote: Total 259 (delta 6), reused 251 (delta 2), pack-reused 0\u001b[K\n","Receiving objects: 100% (259/259), 24.14 MiB | 25.64 MiB/s, done.\n","Resolving deltas: 100% (6/6), done.\n","fatal: not a git repository (or any of the parent directories): .git\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hIG75azoPVk8","executionInfo":{"status":"ok","timestamp":1622047256951,"user_tz":-480,"elapsed":1547,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}}},"source":["# To train own data:\n","# Removing repo data (images) for training/testing/final testing\n","!rm -r /content/object_detection_demo_flow/data/images/final_test/\n","!rm -r /content/object_detection_demo_flow/data/images/train/\n","!rm -r /content/object_detection_demo_flow/data/images/test/"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XA8sl6lPwhm","executionInfo":{"status":"ok","timestamp":1622047297277,"user_tz":-480,"elapsed":481,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}}},"source":["# copying files from gdrive to colab drive.\n","# replacing repo folders with own folders for train, test, and final testing\n","\n","# training folder\n","!cp -r \"/content/inference_saved_model/dataset/train/img\" \"/content/object_detection_demo_flow/data/images/train\"\n","# testing folder\n","!cp -r \"/content/inference_saved_model/dataset/test/img\" \"/content/object_detection_demo_flow/data/images/test\"\n","# final testing folder\n","!cp -r \"/content/inference_saved_model/validation images\" \"/content/object_detection_demo_flow/data/images/final_test\""],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFPLlV3UPwns","executionInfo":{"status":"ok","timestamp":1622047303133,"user_tz":-480,"elapsed":387,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}},"outputId":"60d36637-be5b-430c-bfc7-f5da522688d9"},"source":["# quick check for training data files.\n","!ls /content/object_detection_demo_flow/data/images/train"],"execution_count":8,"outputs":[{"output_type":"stream","text":["image102_jpg.rf.ddcdf035b105b28b570d09058ff9d3b3.jpg\n","image103_jpg.rf.efeb50bcb254f24f93eb53863467b3ee.jpg\n","image103_jpg.rf.f44ba6f21c39e119fb707123dca8f531.jpg\n","image105_jpg.rf.7c445e700ca391130bca704f0c7c039f.jpg\n","image105_jpg.rf.db195a51913fffd4bf43a9ff251bf47f.jpg\n","image106_jpg.rf.26d98ca702dfc7245b17e187c8954286.jpg\n","image106_jpg.rf.ecb93fff2c68e6e259c66ff4b5c31f35.jpg\n","image107_jpg.rf.1f1df261cff24bf989f2096eca9dcb0c.jpg\n","image10_jpg.rf.a0d13ed4b0c8b4f28b264cd637473ea6.jpg\n","image10_jpg.rf.e10c225d8265c8cf5c0b5431af1b9edb.jpg\n","image110_jpg.rf.2c8a6b980451c2f8ac4a6306587b740f.jpg\n","image110_jpg.rf.346632fd806309925906a6533513a4e9.jpg\n","image111_jpg.rf.1839a7f9a15a70e05962697a57494dc0.jpg\n","image111_jpg.rf.8e147747376b576b26dc29810198e78c.jpg\n","image113_jpg.rf.5b622e85ff08e9c38ece2b97fa184599.jpg\n","image113_jpg.rf.a24a34b9001f2829e8cde81c18d071b2.jpg\n","image114_jpg.rf.4ff9f1df1437ee433f55e3ad70c8764c.jpg\n","image114_jpg.rf.9911aae79291dfb1859cfac6ad520de0.jpg\n","image115_jpg.rf.42946965d8901f5753863db6a259e130.jpg\n","image116_jpg.rf.763e3cbe9c729775badcc49a78bac47d.jpg\n","image116_jpg.rf.edd6f9dbefc7438013b460f47041371f.jpg\n","image117_jpg.rf.781d586aaf743c44ec90081a361941b8.jpg\n","image117_jpg.rf.e88ca0284868c82f7390d7e7e2fe8d30.jpg\n","image118_jpg.rf.30bc3da76be9aa28edbe1540e1fcdbe6.jpg\n","image118_jpg.rf.eea5c7820f9dc5ed189d96f6ec97222e.jpg\n","image119_jpg.rf.19455cc5095ed3d02e29a941f5aa0a14.jpg\n","image119_jpg.rf.ba210d49308d0c5e5d6acca0fa793e81.jpg\n","image11_jpg.rf.43333c2fd5ec66d0f04492c2ff9d38f9.jpg\n","image11_jpg.rf.ef6b9be62a8c9b8feae56c5db73a12c4.jpg\n","image120_jpg.rf.cb0cb933775a0c3877eb21b3efbe206e.jpg\n","image120_jpg.rf.f6a79d53389a908a58ff45f1c388effe.jpg\n","image121_jpg.rf.1d985ba95387be88981fdc723c3c86f0.jpg\n","image121_jpg.rf.5062bacb0c772f0e61b68c6fd8587d55.jpg\n","image123_jpg.rf.18f9d6efc00262bdaabdf6e40509ae61.jpg\n","image123_jpg.rf.6d83646fd0b94edb67388709638bf922.jpg\n","image126_jpg.rf.04d94f4aec1c2f84c853c1dd6b779313.jpg\n","image126_jpg.rf.e0a416297863efeac537420d14e94db6.jpg\n","image127_jpg.rf.5f4d612c6bdf84c0f73762196cecae1a.jpg\n","image129_jpg.rf.8d9c5396308fb2071fd219d04651a6b5.jpg\n","image129_jpg.rf.bbebb224e9c600e99d0aacddc354ad4f.jpg\n","image130_jpg.rf.699b0c16f86593b9e00fa9a89fb5f945.jpg\n","image130_jpg.rf.eb232529f9635845b5e23e77fde729d3.jpg\n","image131_jpg.rf.57d918553a81a65473e1472337843f92.jpg\n","image132_jpg.rf.15d1e456c6a5b9176d749c93fe6d2c3c.jpg\n","image132_jpg.rf.3f01e6747fe09e85fede118f1e430916.jpg\n","image134_jpg.rf.8ba833c0faba1af76ba1ba5455bf21a3.jpg\n","image134_jpg.rf.e4fce34ffabb3824a02046fd346ab170.jpg\n","image135_jpg.rf.524153add29fc217b0966fa8fd35b638.jpg\n","image137_jpg.rf.36f9823406275cf6f8c0eda196ab10e8.jpg\n","image137_jpg.rf.3ec099ec0dafffcc0c4c4848cb528e28.jpg\n","image138_jpg.rf.41b8d87f7b5734b4b759a07bf331d3bf.jpg\n","image138_jpg.rf.6bd35cba347ffa291347508f7ed35b1a.jpg\n","image139_jpg.rf.85c9c7df8aaf0cf247517262545df037.jpg\n","image13_jpg.rf.0e3a5e7dec4c42dd81c0ca9dd7d57e9f.jpg\n","image13_jpg.rf.2119afd74a4c58168903ce300f800971.jpg\n","image140_jpg.rf.0ad10a48dd4398690ff4ebe56834b13a.jpg\n","image140_jpg.rf.8fedc9ca73d3a66e49e15eddfea07a0d.jpg\n","image141_jpg.rf.0a2c9576bc8c8366569679f02edaf271.jpg\n","image141_jpg.rf.29892d146fba71324851c9b9067cc7f0.jpg\n","image143_jpg.rf.2a91424b2439c8cfa07e94d0f866583d.jpg\n","image143_jpg.rf.7f991c7eafc5a78ac7c0477cdf81a3d2.jpg\n","image144_jpg.rf.37c7f049e81a86b304d53fd428eb203b.jpg\n","image144_jpg.rf.9e3d187def4b17d1f4757ed36deb95bc.jpg\n","image145_jpg.rf.33c712939522051c19ec4c6d9442ad3a.jpg\n","image145_jpg.rf.e9610751cc2619e56578e11dbf61a313.jpg\n","image146_jpg.rf.325efd2c81752eef900a97168cc63915.jpg\n","image146_jpg.rf.89722b0e92a6f8df9397a2dbf867c96d.jpg\n","image147_jpg.rf.0fbe18b072b4c0ecdd5f408aef90c156.jpg\n","image147_jpg.rf.bb392b2ca68fbe8482fbfbd2487364bd.jpg\n","image148_jpg.rf.338e7df485fb02ab57b2d388a40ec52b.jpg\n","image148_jpg.rf.aa2e5828108307247d44835407a9cc0b.jpg\n","image14_jpg.rf.d11f7155c5ab886a6a1b866ead5d23f4.jpg\n","image150_jpg.rf.305ee2fdc6734065d58fbc2fea28de5b.jpg\n","image150_jpg.rf.a465ae0fc604095320c5494b0befc6b7.jpg\n","image15_jpg.rf.2df8e7d99d4297d2b9fadd169650b71d.jpg\n","image15_jpg.rf.3c354408bd73804afc0827b00f47fb43.jpg\n","image16_jpg.rf.54378f0d84f52b93c831c38bb25df6c4.jpg\n","image16_jpg.rf.8dd3b4c3aa09911d88cfef7faaf1a7ad.jpg\n","image17_jpg.rf.effdaf514398bbd73ee3d6ba540462f0.jpg\n","image18_jpg.rf.68854b52fd1a34e4bbf3464780668a4c.jpg\n","image18_jpg.rf.a3407051983f07032c66358a1a32e343.jpg\n","image19_jpg.rf.27de82617e3b3eac969b7a255483c4c8.jpg\n","image19_jpg.rf.617154c49e9f4ce7419be4b33102734d.jpg\n","image1_jpg.rf.56fe50b3b9dff506a33d1881d9385a34.jpg\n","image1_jpg.rf.be30f774faead56e1ca3f7ffd3804e96.jpg\n","image20_jpg.rf.496e8c32bdb14a68600cc9e485c7f7e1.jpg\n","image20_jpg.rf.9bdcba7b42c07f1458a59aa07258d2d1.jpg\n","image21_jpg.rf.1f55bb0d9981e16347967ce895735a6f.jpg\n","image21_jpg.rf.56bc836e287c3e6c1deca0d5468b4712.jpg\n","image22_jpg.rf.5172f1ea8f63d9c44d0f1c1a71e29851.jpg\n","image22_jpg.rf.e005daa530924d25bc44fd45000599b2.jpg\n","image23_jpg.rf.b1357d736720b09452bd5bdb0b887298.jpg\n","image25_jpg.rf.7fc060871bf1a7b10050f7330a0c41d1.jpg\n","image25_jpg.rf.813df4966f86bafb12add080678cbe8a.jpg\n","image26_jpg.rf.14bc1ab63d7d28069b49e7e4249b7bf9.jpg\n","image26_jpg.rf.ec5b78f97a600e86fe43b51548239b2b.jpg\n","image27_jpg.rf.0e2c8cb6f6d576abd7ec72c1026f2dbc.jpg\n","image27_jpg.rf.f02aa953a5c6f98ca1c2faf7e27536a6.jpg\n","image28_jpg.rf.13a6102913ca3b22252caab3fe444f9e.jpg\n","image29_jpg.rf.3ca473a042f9d536b2f5419d2a13cce2.jpg\n","image29_jpg.rf.e20a682103a6ab4a0f7deb172fcf3f5d.jpg\n","image30_jpg.rf.91d4c6c268b33a62aaa30de226dbf1c0.jpg\n","image30_jpg.rf.fbc7deefe336cce963e1b1e6d114d7b0.jpg\n","image32_jpg.rf.2e6902fac50ae01790b07707e5822b0a.jpg\n","image32_jpg.rf.77cea8c8f5926834fa3ef6d2de5d2290.jpg\n","image33_jpg.rf.c39ab34b13ad72dc84054cccbfcf4dfe.jpg\n","image33_jpg.rf.e06320fcff1dfb095cbbfda4f053c1ac.jpg\n","image38_jpg.rf.1a1a109b09cfbd177973a8343a08d086.jpg\n","image3_jpg.rf.4e2553f83728ba08206baa4ffef78cc4.jpg\n","image3_jpg.rf.fa167b529aa656a9b7abf95d6f7c7faf.jpg\n","image41_jpg.rf.530b471b8dcb0775a6d618bb2969caad.jpg\n","image41_jpg.rf.f8ab0c2fb71d65071f095e5b7cff4ba6.jpg\n","image42_jpg.rf.39937663be978df925efd419d40e7ac7.jpg\n","image42_jpg.rf.974fba73c539d0cc6ac0bfc4cc4b8a4b.jpg\n","image43_jpg.rf.efab7d302bc942000b74ba260707eb91.jpg\n","image43_jpg.rf.f6245a39a55fda8888dfef33e4c1fb8b.jpg\n","image44_jpg.rf.5a33ab8cdc179eac574565fcd80cac73.jpg\n","image44_jpg.rf.d0e3f1af8bc2fca04ee4c01eba06b7e5.jpg\n","image45_jpg.rf.4b718b7113bcc9085fb847a2301122bb.jpg\n","image46_jpg.rf.f7f857a9014d35e0615c4c3f16ac84b1.jpg\n","image47_jpg.rf.429f59c64d4510506ff4c8174c1ee225.jpg\n","image50_jpg.rf.de6b7c77359ce32ea0f2ceff5c0c773e.jpg\n","image51_jpg.rf.1758643bc1ded7deb97abee7e0e34f5a.jpg\n","image51_jpg.rf.32d3efaa2fd6c8771c81a4b7cac91215.jpg\n","image52_jpg.rf.7fdc2146d00e6d849e9073615f82b1c6.jpg\n","image52_jpg.rf.de08cdf6679ceb6b2c5a8ff99d916b36.jpg\n","image54_jpg.rf.71d418687ee91c25c0e3f0fdee0cb095.jpg\n","image54_jpg.rf.d1e8da8dcfd195f0d3b35b7977358d9e.jpg\n","image55_jpg.rf.97b452a0f561ead431fafc10d6417929.jpg\n","image56_jpg.rf.8245ea2e306f2b0a0feaf860a864cf15.jpg\n","image56_jpg.rf.f0b0d23989e5855ffcb4dbccfc966c49.jpg\n","image59_jpg.rf.70a7ba0c4b322575c150901fcf58b1a1.jpg\n","image59_jpg.rf.bbce2854c3ef33760f9d5c7f67818cf6.jpg\n","image61_jpg.rf.d446c73b6a04295cdfcdbea7431d5015.jpg\n","image62_jpg.rf.51092faa93c6e81927d93b622304b51c.jpg\n","image62_jpg.rf.b0d63613205238583039cc59735f850c.jpg\n","image63_jpg.rf.c7c7083a3647eb41444079d697e8acb3.jpg\n","image65_jpg.rf.aaa6db458f76510f3150dbf625480b0e.jpg\n","image65_jpg.rf.ef4c4c8db228305483b7a1813cfa7c3d.jpg\n","image66_jpg.rf.2455af781b08ff46f588e699fe49b2e3.jpg\n","image66_jpg.rf.6b2c46901f9202d885c90f55e1b73be2.jpg\n","image67_jpg.rf.2b0e171c25c35c14b0acd91eeea0b500.jpg\n","image68_jpg.rf.5e9993ceee79be905f86a120f3878e50.jpg\n","image69_jpg.rf.d495b39f89ff889d815e76131580ad4f.jpg\n","image71_jpg.rf.6aa869d8c400293015bb4496b7ae4169.jpg\n","image72_jpg.rf.186a5562d36a963d4b3dc821a44a953b.jpg\n","image72_jpg.rf.d5e98b41c499fd67af73668cffadfe03.jpg\n","image73_jpg.rf.466cfd5edca58016750a12e6899441cc.jpg\n","image74_jpg.rf.33bc8085e867fd7af2b46c020b3c00bc.jpg\n","image74_jpg.rf.8ea5d194c719827eecdd5f5f0509fa8c.jpg\n","image75_jpg.rf.04602d3648101d0370f7997d38faf787.jpg\n","image75_jpg.rf.a7ee88b152d275ed77ae7a235b31f753.jpg\n","image76_jpg.rf.01560fd13046ecd2e13b278da8a64ab7.jpg\n","image76_jpg.rf.4bf356cb3643dd1a05293f7012685625.jpg\n","image77_jpg.rf.5ddf2865e74b99c83b28b6ca74b0e920.jpg\n","image77_jpg.rf.79e194bd6a851658e6833fe949c1ebfd.jpg\n","image78_jpg.rf.328411a81e287237a1eb91b1cc69b34f.jpg\n","image78_jpg.rf.a32f148aaee0993d6048e4d87b0fff52.jpg\n","image79_jpg.rf.bb824316dace2403b45adf933e19f1ec.jpg\n","image80_jpg.rf.41a45486834fe32b5c714dcf31fd2165.jpg\n","image81_jpg.rf.c9824657bad2faa516e67b41920e37b2.jpg\n","image82_jpg.rf.35f842dc4b9c3d4f4a494c6a87ad90ff.jpg\n","image82_jpg.rf.3896b94286c9968514213db0a937da41.jpg\n","image84_jpg.rf.6dbfbc1556b6abc88b71cf00fb66f713.jpg\n","image84_jpg.rf.9c1f0860007d53fea8ad8787c7fc6587.jpg\n","image86_jpg.rf.704d1d0875ae3a9db36abba4e6f831e9.jpg\n","image87_jpg.rf.055378b509b1258736ffc8aac7faf765.jpg\n","image87_jpg.rf.46fea4beb2b4b17cf9aad12277160f12.jpg\n","image88_jpg.rf.61459ff3f3de36064b51c91ee82dc74f.jpg\n","image89_jpg.rf.55dd4e6e109e128588219cb5b14d2fae.jpg\n","image89_jpg.rf.a429c2c84fb16a37f48a2759c0021ce5.jpg\n","image8_jpg.rf.7747e162d7387d5f66335b8cfef687e4.jpg\n","image8_jpg.rf.a59ea70ce795bffdd06587175d05ba46.jpg\n","image95_jpg.rf.8ebcf48764b72c034057fd6cadc25659.jpg\n","image97_jpg.rf.d76ff14b92b1f9fdb503a62b33fbcaa1.jpg\n","image98_jpg.rf.22959e6abb0673c355079af55e5fff77.jpg\n","image99_jpg.rf.ca0b03103c86af16548ac9896b58b6f3.jpg\n","image99_jpg.rf.f31a027c4a1b1b2c1e2a3acce6e94616.jpg\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"unIgOtV-Pwtk","executionInfo":{"status":"ok","timestamp":1622047363860,"user_tz":-480,"elapsed":56339,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}}},"source":["# installing Tensorflow object detection API\n","%%capture\n","%cd /content\n","!git clone --quiet https://github.com/tensorflow/models.git\n","%cd /content/models/\n","!git checkout 58d19c67e1d30d905dd5c6e5092348658fed80af\n","!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n","!pip install -q Cython contextlib2 pillow lxml matplotlib\n","!pip install -q pycocotools\n","%cd /content/models/research\n","!protoc object_detection/protos/*.proto --python_out=.\n","import os\n","os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n","!python object_detection/builders/model_builder_test.py"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fz52pLO7PwzN"},"source":["# Comment this cell out if tfrecord and label map files are saved already\n","'''\n","# preparing tfrecord files\n","%%capture\n","%cd {repo_dir_path}\n","\n","# Convert train folder annotation xml files to a single csv file,\n","# generate the `label_map.pbtxt` file to `data/` directory as well.\n","!python xml_to_csv.py -i data/images/train -o data/annotations/train_labels.csv -l data/annotations\n","\n","# Convert test folder annotation xml files to a single csv.\n","!python xml_to_csv.py -i data/images/test -o data/annotations/test_labels.csv\n","\n","# Generate `train.record`\n","!python generate_tfrecord.py --csv_input=data/annotations/train_labels.csv --output_path=data/annotations/train.record --img_path=data/images/train --label_map data/annotations/label_map.pbtxt\n","\n","# Generate `test.record`\n","!python generate_tfrecord.py --csv_input=data/annotations/test_labels.csv --output_path=data/annotations/test.record --img_path=data/images/test --label_map data/annotations/label_map.pbtxt\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wbf_zySegnbc","executionInfo":{"status":"ok","timestamp":1622047424920,"user_tz":-480,"elapsed":616,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}}},"source":["# Set the paths\n","test_record_fname = \"/content/inference_saved_model/dataset/test/person.tfrecord\"\n","train_record_fname = \"/content/inference_saved_model/dataset/train/person.tfrecord\"\n","label_map_pbtxt_fname = \"/content/inference_saved_model/dataset/train/person_label_map.pbtxt\""],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"didv-0hyPw4g","executionInfo":{"status":"ok","timestamp":1622047435535,"user_tz":-480,"elapsed":3504,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}},"outputId":"6873d8f4-1f86-4220-ee8e-6f2fdf480194"},"source":["# downloading ssd mobilenet v2 model\n","%cd /content/models/research\n","\n","import os\n","import shutil\n","import glob\n","import urllib.request\n","import tarfile\n","MODEL_FILE = MODEL + '.tar.gz'\n","DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n","DEST_DIR = '/content/models/research/pretrained_model'\n","\n","if not (os.path.exists(MODEL_FILE)):\n","    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n","\n","tar = tarfile.open(MODEL_FILE)\n","tar.extractall()\n","tar.close()\n","\n","os.remove(MODEL_FILE)\n","if (os.path.exists(DEST_DIR)):\n","    shutil.rmtree(DEST_DIR)\n","os.rename(MODEL, DEST_DIR)\n","!echo {DEST_DIR}\n","!ls -alh {DEST_DIR}"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/content/models/research\n","/content/models/research/pretrained_model\n","total 135M\n","drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 .\n","drwxr-xr-x 63 root   root  4.0K May 26 16:43 ..\n","-rw-r--r--  1 345018 89939   77 Mar 30  2018 checkpoint\n","-rw-r--r--  1 345018 89939  67M Mar 30  2018 frozen_inference_graph.pb\n","-rw-r--r--  1 345018 89939  65M Mar 30  2018 model.ckpt.data-00000-of-00001\n","-rw-r--r--  1 345018 89939  15K Mar 30  2018 model.ckpt.index\n","-rw-r--r--  1 345018 89939 3.4M Mar 30  2018 model.ckpt.meta\n","-rw-r--r--  1 345018 89939 4.2K Mar 30  2018 pipeline.config\n","drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 saved_model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ohjQVSBSPw_f","executionInfo":{"status":"ok","timestamp":1622047441962,"user_tz":-480,"elapsed":752,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}},"outputId":"0d5e024b-5abd-4546-9dfa-17ba97de22df"},"source":["# TF pretrained model checkpoint\n","fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n","fine_tune_checkpoint"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/models/research/pretrained_model/model.ckpt'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"qjvmttwnRrsP","executionInfo":{"status":"ok","timestamp":1622047445163,"user_tz":-480,"elapsed":319,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}}},"source":["# Configuring training pipeline\n","import os\n","pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n","\n","assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)\n","def get_num_classes(pbtxt_fname):\n","    from object_detection.utils import label_map_util\n","    label_map = label_map_util.load_labelmap(pbtxt_fname)\n","    categories = label_map_util.convert_label_map_to_categories(\n","        label_map, max_num_classes=90, use_display_name=True)\n","    category_index = label_map_util.create_category_index(categories)\n","    return len(category_index.keys())"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4vISsYXRr07","executionInfo":{"status":"ok","timestamp":1622047448906,"user_tz":-480,"elapsed":860,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}}},"source":["import re\n","iou_threshold = 0.50\n","num_classes = get_num_classes(label_map_pbtxt_fname)\n","with open(pipeline_fname) as f:\n","    s = f.read()\n","with open(pipeline_fname, 'w') as f:\n","    \n","    # fine_tune_checkpoint\n","    s = re.sub('fine_tune_checkpoint: \".*?\"',\n","               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n","    \n","    # tfrecord files train and test.\n","    s = re.sub(\n","        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n","    s = re.sub(\n","        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n","\n","    # label_map_path\n","    s = re.sub(\n","        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n","\n","    # Set training batch_size.\n","    s = re.sub('batch_size: [0-9]+',\n","               'batch_size: {}'.format(batch_size), s)\n","\n","    # Set training steps, num_steps\n","    s = re.sub('num_steps: [0-9]+',\n","               'num_steps: {}'.format(num_steps), s)\n","    \n","    # Set number of classes num_classes.\n","    s = re.sub('num_classes: [0-9]+',\n","               'num_classes: {}'.format(num_classes), s)\n","    # Set number of classes num_classes.\n","    s = re.sub('iou_threshold: [0-9].[0-9]+',\n","               'iou_threshold: {}'.format(iou_threshold), s)\n","    \n","    f.write(s)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLu3VERuRr9P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621504461172,"user_tz":-480,"elapsed":7600,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}},"outputId":"3869b240-b646-4f77-931f-17d55d3ec139"},"source":["# Add Tensorboard visualisation to the training process\n","# After running this cell click on the link in the output cell to open tensorboard\n","# Tensoarboard will show you graphically different training parameters as the model is training\n","# when training finishes after the set number of steps, tensorboard can be used to see a nice summary of the training process\n","# Visuals will load in Tensorboard after the model has gone through a few hundred steps\n","\n","model_dir = 'training/'\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip\n","LOG_DIR = model_dir\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","get_ipython().system_raw('./ngrok http 6006 &')\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-05-20 09:54:15--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 54.205.76.237, 34.233.212.111, 3.216.229.131, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|54.205.76.237|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13832437 (13M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip.3’\n","\n","ngrok-stable-linux- 100%[===================>]  13.19M  6.68MB/s    in 2.0s    \n","\n","2021-05-20 09:54:17 (6.68 MB/s) - ‘ngrok-stable-linux-amd64.zip.3’ saved [13832437/13832437]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: ngrok                   \n","http://5c873d873746.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mIEAeMqMRsFc"},"source":["# Start the training\n","model_dir = 'training/'\n","# Optionally remove content in output model directory for a fresh start.\n","!rm -rf {model_dir}\n","os.makedirs(model_dir, exist_ok=True)\n","!python /content/models/research/object_detection/model_main.py \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --model_dir={model_dir} \\\n","    --alsologtostderr \\\n","    --num_train_steps={num_steps} \\\n","    --num_eval_steps={num_eval_steps}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"smH1zfU2RsNQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621504717896,"user_tz":-480,"elapsed":1153,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}},"outputId":"c768e4c1-3dc5-45ad-d2c5-f91be14088a8"},"source":["# model dir check for the trained model\n","!ls {model_dir}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["checkpoint\n","eval_0\n","events.out.tfevents.1621498620.afe1ffe982ee\n","export\n","graph.pbtxt\n","model.ckpt-11706.data-00000-of-00001\n","model.ckpt-11706.index\n","model.ckpt-11706.meta\n","model.ckpt-14077.data-00000-of-00001\n","model.ckpt-14077.index\n","model.ckpt-14077.meta\n","model.ckpt-16445.data-00000-of-00001\n","model.ckpt-16445.index\n","model.ckpt-16445.meta\n","model.ckpt-18824.data-00000-of-00001\n","model.ckpt-18824.index\n","model.ckpt-18824.meta\n","model.ckpt-20000.data-00000-of-00001\n","model.ckpt-20000.index\n","model.ckpt-20000.meta\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p1R63_TdPVtD"},"source":["# clean output_directory if necessary to start fresh:\n","\n","!rm -rf /content/object_detection_demo/fine_tuned_model/ \n","os.makedirs('/content/object_detection_demo_flow/fine_tuned_model/', exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNe8HuoYSzxT"},"source":["# Export trained inference graph\n","%%capture\n","import re\n","import numpy as np\n","\n","output_directory = './fine_tuned_model'\n","# output_directory = '/content/gdrive/My\\ Drive/data/'\n","\n","lst = os.listdir(model_dir)\n","lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n","steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n","last_model = lst[steps.argmax()].replace('.meta', '')\n","\n","last_model_path = os.path.join(model_dir, last_model)\n","print(last_model_path)\n","!python /content/models/research/object_detection/export_inference_graph.py \\\n","    --input_type=image_tensor \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --output_directory={output_directory} \\\n","    --trained_checkpoint_prefix={last_model_path}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sUCDZdJRSz6T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621504809976,"user_tz":-480,"elapsed":1168,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}},"outputId":"3fd773a8-8259-41f4-dbbd-b17929b6c9fb"},"source":["# export directory check\n","!ls {output_directory}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["checkpoint\t\t\tmodel.ckpt.index  saved_model\n","frozen_inference_graph.pb\tmodel.ckpt.meta\n","model.ckpt.data-00000-of-00001\tpipeline.config\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4MR0MiFbS0Ct"},"source":["import os\n","pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")\n","assert os.path.isfile(pb_fname), '`{}` not exist'.format(pb_fname)\n","# !ls -alh {pb_fname}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vuEwyTQqS0LH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621504814410,"user_tz":-480,"elapsed":1076,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}},"outputId":"f0ecce22-7455-41e5-e020-1279261d21be"},"source":["# Running inference to check what the model can detect\n","import os\n","import glob\n","\n","# Path to frozen detection graph. This is the actual model that is used for the object detection.\n","PATH_TO_CKPT = pb_fname\n","\n","# List of the strings that is used to add correct label for each box.\n","PATH_TO_LABELS = label_map_pbtxt_fname\n","\n","# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n","PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, \"data/images/final_test\")\n","\n","assert os.path.isfile(pb_fname)\n","assert os.path.isfile(PATH_TO_LABELS)\n","TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.*\"))\n","assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n","print(TEST_IMAGE_PATHS)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['/content/object_detection_demo_flow/data/images/final_test/image5.jpg', '/content/object_detection_demo_flow/data/images/final_test/image9.jpg', '/content/object_detection_demo_flow/data/images/final_test/image3.jpg', '/content/object_detection_demo_flow/data/images/final_test/image8.jpg', '/content/object_detection_demo_flow/data/images/final_test/image6.jpg', '/content/object_detection_demo_flow/data/images/final_test/image4.jpg', '/content/object_detection_demo_flow/data/images/final_test/image10.jpg', '/content/object_detection_demo_flow/data/images/final_test/image1.jpg', '/content/object_detection_demo_flow/data/images/final_test/image2.jpg', '/content/object_detection_demo_flow/data/images/final_test/image7.jpg']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bRyY3muzS0U9","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1_vSt8a5dp43HQkqEK8xv2DHHLosvdepO"},"executionInfo":{"status":"ok","timestamp":1621505274415,"user_tz":-480,"elapsed":21566,"user":{"displayName":"Arthur Chin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRdB98tHHjC-b0VMh6Q0wKuH93BNUaUVSSFP3BjQ=s64","userId":"01992116258059764651"}},"outputId":"c2203324-2936-4d2e-80fc-cd8f2d488dc2"},"source":["%cd /content/models/research/object_detection\n","\n","import numpy as np\n","import os\n","import six.moves.urllib as urllib\n","import sys\n","import tarfile\n","import tensorflow as tf\n","import zipfile\n","\n","from collections import defaultdict\n","from io import StringIO\n","# This is needed to display the images.\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","# This is needed since the notebook is stored in the object_detection folder.\n","sys.path.append(\"..\")\n","from object_detection.utils import ops as utils_ops\n","\n","from object_detection.utils import label_map_util\n","\n","from object_detection.utils import visualization_utils as vis_util\n","\n","\n","detection_graph = tf.Graph()\n","with detection_graph.as_default():\n","    od_graph_def = tf.GraphDef()\n","    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n","        serialized_graph = fid.read()\n","        od_graph_def.ParseFromString(serialized_graph)\n","        tf.import_graph_def(od_graph_def, name='')\n","\n","\n","label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n","categories = label_map_util.convert_label_map_to_categories(\n","    label_map, max_num_classes=num_classes, use_display_name=True)\n","category_index = label_map_util.create_category_index(categories)\n","\n","\n","def load_image_into_numpy_array(image):\n","    (im_width, im_height) = image.size\n","    return np.array(image.getdata()).reshape(\n","        (im_height, im_width, 3)).astype(np.uint8)\n","\n","# Size, in inches, of the output images.\n","IMAGE_SIZE = (12, 8)\n","\n","\n","def run_inference_for_single_image(image, graph):\n","    with graph.as_default():\n","        with tf.Session() as sess:\n","            # Get handles to input and output tensors\n","            ops = tf.get_default_graph().get_operations()\n","            all_tensor_names = {\n","                output.name for op in ops for output in op.outputs}\n","            tensor_dict = {}\n","            for key in [\n","                'num_detections', 'detection_boxes', 'detection_scores',\n","                'detection_classes', 'detection_masks'\n","            ]:\n","                tensor_name = key + ':0'\n","                if tensor_name in all_tensor_names:\n","                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n","                        tensor_name)\n","            if 'detection_masks' in tensor_dict:\n","                # The following processing is only for single image\n","                detection_boxes = tf.squeeze(\n","                    tensor_dict['detection_boxes'], [0])\n","                detection_masks = tf.squeeze(\n","                    tensor_dict['detection_masks'], [0])\n","                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n","                real_num_detection = tf.cast(\n","                    tensor_dict['num_detections'][0], tf.int32)\n","                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n","                                           real_num_detection, -1])\n","                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n","                                           real_num_detection, -1, -1])\n","                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n","                detection_masks_reframed = tf.cast(\n","                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n","                # Follow the convention by adding back the batch dimension\n","                tensor_dict['detection_masks'] = tf.expand_dims(\n","                    detection_masks_reframed, 0)\n","            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n","\n","            # Run inference\n","            output_dict = sess.run(tensor_dict,\n","                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n","\n","            # all outputs are float32 numpy arrays, so convert types as appropriate\n","            output_dict['num_detections'] = int(\n","                output_dict['num_detections'][0])\n","            output_dict['detection_classes'] = output_dict[\n","                'detection_classes'][0].astype(np.uint8)\n","            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n","            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n","            if 'detection_masks' in output_dict:\n","                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n","    return output_dict\n","\n","\n","for image_path in TEST_IMAGE_PATHS:\n","    image = Image.open(image_path)\n","    print(image_path)\n","    # the array based representation of the image will be used later in order to prepare the\n","    # result image with boxes and labels on it.\n","    image_np = load_image_into_numpy_array(image)\n","    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n","    image_np_expanded = np.expand_dims(image_np, axis=0)\n","    # Actual detection.\n","    output_dict = run_inference_for_single_image(image_np, detection_graph)\n","    # Visualization of the results of a detection.\n","    vis_util.visualize_boxes_and_labels_on_image_array(\n","        image_np,\n","        output_dict['detection_boxes'],\n","        output_dict['detection_classes'],\n","        output_dict['detection_scores'],\n","        category_index,\n","        instance_masks=output_dict.get('detection_masks'),\n","        use_normalized_coordinates=True,\n","        line_thickness=2,\n","        skip_scores=True,\n","        skip_labels=True)\n","    plt.figure(figsize=IMAGE_SIZE)\n","    plt.imshow(image_np)\n","    plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}